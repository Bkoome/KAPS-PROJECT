{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZSldBluFuXPuIsaWTEpvukBWQN__R_Ne",
      "authorship_tag": "ABX9TyPUxdxKSkh8ZAeAM8GGKY7d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bkoome/KAPS-PROJECT/blob/main/U_Net_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "84d3HoMTxVTw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/drive/MyDrive/potato_project/Training'\n",
        "mask_dir = '/content/drive/MyDrive/potato_project/Training'"
      ],
      "metadata": {
        "id": "v4bC8SqNxeH-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_and_preprocess_data(image_dir, mask_dir, target_size=(256, 256)):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    for file in image_files:\n",
        "        if file.endswith('.jpg'):  # Assuming images are in JPG format\n",
        "            # Load and preprocess image\n",
        "            image_path = os.path.join(image_dir, file)\n",
        "            image = load_img(image_path, target_size=target_size)\n",
        "            image = img_to_array(image) / 255.0  # Normalize pixel values\n",
        "            images.append(image)\n",
        "\n",
        "            # Load and preprocess mask (assuming masks are in a separate directory)\n",
        "            mask_path = os.path.join(mask_dir, file.replace('.jpg', '_mask.jpg'))\n",
        "            mask = load_img(mask_path, target_size=target_size, color_mode='grayscale')\n",
        "            mask = img_to_array(mask) / 255.0\n",
        "            masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_and_preprocess_data(image_dir, mask_dir)"
      ],
      "metadata": {
        "id": "B4T6yNTczSBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode mask labels (assuming the masks are grayscale)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y.flatten())\n",
        "y_categorical = to_categorical(y_encoded, num_classes=3)  # Update num_classes to 3\n"
      ],
      "metadata": {
        "id": "WIzuGRmoBn-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "AHLrBxV2D4zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "l95q3FpMD_Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the U-Net model\n",
        "def unet_model(input_shape=(256, 256, 3), num_classes=3):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Contracting Path\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    # Expanding Path\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    up1 = layers.UpSampling2D(size=(2, 2))(conv2)\n",
        "    up1 = layers.concatenate([conv1, up1], axis=-1)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(up1)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name='unet_model')\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "model = unet_model()"
      ],
      "metadata": {
        "id": "BtmyptR1ESma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "YBolX_XWEflu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(X_train) // batch_size,\n",
        "                    epochs=10,\n",
        "                    validation_data=datagen.flow(X_val, y_val, batch_size=batch_size),\n",
        "                    validation_steps=len(X_val) // batch_size)\n"
      ],
      "metadata": {
        "id": "ykEw-TdKFDkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `image_to_predict` is the new orthomosaic image you want to predict\n",
        "predicted_masks = model.predict(np.expand_dims(image_to_predict, axis=0))"
      ],
      "metadata": {
        "id": "Kx73JluAFTVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `color_mapping` is a dictionary mapping class indices to colors\n",
        "color_mapping = {\n",
        "    0: [255, 0, 0],  # Red for Early Blight\n",
        "    1: [0, 255, 0],  # Green for Late Blight\n",
        "    2: [0, 0, 255]   # Blue for Healthy Crops\n",
        "}"
      ],
      "metadata": {
        "id": "rKOaRLfXFUk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overlay colors on the original image based on predicted masks\n",
        "segmented_image = np.zeros_like(image_to_predict)\n",
        "\n",
        "for class_index, color in color_mapping.items():\n",
        "    class_mask = predicted_masks[..., class_index]\n",
        "    segmented_image[class_mask > 0.5] = color\n",
        "\n",
        "# Display or save the segmented image"
      ],
      "metadata": {
        "id": "UeFbQNZdFsl9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}